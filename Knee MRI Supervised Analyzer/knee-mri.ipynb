{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import glob\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Paths for each view (update these paths with the correct file locations)\n",
    "train_axial_path = 'train/axial/'\n",
    "valid_axial_path = 'valid/axial/'\n",
    "train_coronal_path = 'train/coronal/'\n",
    "valid_coronal_path = 'valid/coronal/'\n",
    "train_sagittal_path = 'train/sagittal/'\n",
    "valid_sagittal_path = 'valid/sagittal/'\n",
    "\n",
    "# CSV paths for labels (update these paths with the correct file locations)\n",
    "train_labels_abnormal = 'train-abnormal.csv'\n",
    "valid_labels_abnormal = 'valid-abnormal.csv'\n",
    "train_labels_acl = 'train-acl.csv'\n",
    "valid_labels_acl = 'valid-acl.csv'\n",
    "train_labels_meniscus = 'train-meniscus.csv'\n",
    "valid_labels_meniscus = 'valid-meniscus.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for my MRIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, axial_path, coronal_path, sagittal_path, abnormal_labels_path, acl_labels_path, meniscus_labels_path, transform=None):\n",
    "        # Collecting all file paths for each orientation into one list\n",
    "        self.all_images = sorted(glob.glob(os.path.join(axial_path, '*.npy')) +\n",
    "                                 glob.glob(os.path.join(coronal_path, '*.npy')) +\n",
    "                                 glob.glob(os.path.join(sagittal_path, '*.npy')))\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load labels\n",
    "        abnormal_labels = pd.read_csv(abnormal_labels_path, header=None)[1].to_numpy()\n",
    "        acl_labels = pd.read_csv(acl_labels_path, header=None)[1].to_numpy()\n",
    "        meniscus_labels = pd.read_csv(meniscus_labels_path, header=None)[1].to_numpy()\n",
    "\n",
    "        # Stack labels to create a (num_samples, 3) array (1 label per sample)\n",
    "        self.labels = np.vstack((abnormal_labels, acl_labels, meniscus_labels)).T\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is the total number of images (across all orientations)\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image from the unified list of all images\n",
    "        image = np.load(self.all_images[idx])\n",
    "\n",
    "        # Take the middle slice if images have 3 dimensions\n",
    "        def middle_slice(image):\n",
    "            return image[image.shape[0] // 2] if image.ndim == 3 else image\n",
    "\n",
    "        image = middle_slice(image)\n",
    "\n",
    "        # Convert the image to PIL format and apply transformations if any\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))  # Rescale to [0, 255] for display\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply any transform passed\n",
    "\n",
    "        # For labels, we return the label corresponding to the current index\n",
    "        label = torch.tensor(self.labels[idx % len(self.labels)], dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and augmentation transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),                   # Resize to 128x128\n",
    "    transforms.ToTensor(),                           # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])      # Normalize with mean=0.5, std=0.5 (assuming grayscale)\n",
    "])\n",
    "\n",
    "# Additional augmentation transforms\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),           # Randomly rotate image by up to 10 degrees\n",
    "    transforms.RandomHorizontalFlip(),               # Randomly flip horizontally\n",
    "    preprocess_transforms                            # Apply preprocessing after augmentations\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dataset with augmentation transforms for training\n",
    "train_dataset = MRIDataset(\n",
    "    axial_path=train_axial_path, coronal_path=train_coronal_path, sagittal_path=train_sagittal_path,\n",
    "    abnormal_labels_path=train_labels_abnormal, acl_labels_path=train_labels_acl, meniscus_labels_path=train_labels_meniscus,\n",
    "    transform=augmentation_transforms\n",
    ")\n",
    "\n",
    "# Initialize DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = MRIDataset(\n",
    "    axial_path=valid_axial_path, coronal_path=valid_coronal_path, sagittal_path=valid_sagittal_path,\n",
    "    abnormal_labels_path=valid_labels_abnormal, acl_labels_path=valid_labels_acl, meniscus_labels_path=valid_labels_meniscus,\n",
    "    transform=preprocess_transforms\n",
    ")\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My simple convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3)  # 3 output classes for multi-label classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "model.to(device)  # Move model to GPU if available\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use binary cross-entropy for multi-label classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    dataset_size = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
    "    for step, (images, labels) in bar:\n",
    "        # We take the images and their labels and push them on GPU\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Obtain predictions\n",
    "        pred = model(images)\n",
    "\n",
    "        # Compute loss for this batch\n",
    "        loss = criterion(pred, labels)\n",
    "\n",
    "        # Compute gradients for each weight (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights based on gradients (gradient descent)\n",
    "        optimizer.step()\n",
    "\n",
    "        # We keep track of the average training loss\n",
    "        total_train_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, dataloader, criterion, epoch):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    dataset_size = 0\n",
    "    correct = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
    "\n",
    "    for step, (images, labels) in bar:  # Expect two values: images and labels\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float()  # Ensure labels are floats for BCEWithLogitsLoss\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        pred = model(images)  # Predictions should have shape [batch_size, num_classes]\n",
    "        loss = criterion(pred, labels)  # Ensure labels have same shape as pred\n",
    "\n",
    "        # For evaluation: Convert predictions to binary (0/1) using threshold\n",
    "        predictions = (torch.sigmoid(pred) > 0.5).int()\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        correct += (predictions == labels.int()).sum().item()\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        total_val_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
    "        accuracy = np.round(100 * correct / (dataset_size * labels.size(1)), 2)  # Normalize by total number of elements\n",
    "\n",
    "        bar.set_postfix(Epoch=epoch, Valid_Acc=accuracy, Valid_Loss=epoch_loss)\n",
    "\n",
    "    return accuracy, epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:08<00:00, 12.37it/s, Epoch=0, Train_Loss=0.54]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 16.97it/s, Epoch=0, Valid_Acc=63.4, Valid_Loss=0.75]\n",
      "Validation Accuracy Improved (0.0 ---> 63.43)\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:04<00:00, 23.15it/s, Epoch=1, Train_Loss=0.51]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 26.63it/s, Epoch=1, Valid_Acc=65.7, Valid_Loss=0.63]\n",
      "Validation Accuracy Improved (63.43 ---> 65.65)\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 28.32it/s, Epoch=2, Train_Loss=0.5] \n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 51.05it/s, Epoch=2, Valid_Acc=66.3, Valid_Loss=0.63]\n",
      "Validation Accuracy Improved (65.65 ---> 66.3)\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 29.44it/s, Epoch=3, Train_Loss=0.49]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 55.37it/s, Epoch=3, Valid_Acc=67.1, Valid_Loss=0.62]\n",
      "Validation Accuracy Improved (66.3 ---> 67.13)\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 29.45it/s, Epoch=4, Train_Loss=0.47]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 53.23it/s, Epoch=4, Valid_Acc=67, Valid_Loss=0.61] \n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 28.95it/s, Epoch=5, Train_Loss=0.47]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 54.69it/s, Epoch=5, Valid_Acc=67.9, Valid_Loss=0.64]\n",
      "Validation Accuracy Improved (67.13 ---> 67.87)\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 28.90it/s, Epoch=6, Train_Loss=0.45]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 52.26it/s, Epoch=6, Valid_Acc=69.4, Valid_Loss=0.62]\n",
      "Validation Accuracy Improved (67.87 ---> 69.44)\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 28.00it/s, Epoch=7, Train_Loss=0.44]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 52.62it/s, Epoch=7, Valid_Acc=69.3, Valid_Loss=0.61]\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 28.19it/s, Epoch=8, Train_Loss=0.44]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 53.90it/s, Epoch=8, Valid_Acc=70.9, Valid_Loss=0.59]\n",
      "Validation Accuracy Improved (69.44 ---> 70.93)\n",
      "\n",
      "100%|\u001b[36m██████████\u001b[0m| 106/106 [00:03<00:00, 27.83it/s, Epoch=9, Train_Loss=0.43]\n",
      "100%|\u001b[36m██████████\u001b[0m| 12/12 [00:00<00:00, 52.99it/s, Epoch=9, Valid_Acc=70, Valid_Loss=0.65] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_training(model, criterion, trainloader, testloader, optimizer, num_epochs):\n",
    "    # Check if we are using GPU\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "    # For keeping track of the best validation accuracy\n",
    "    top_accuracy = 0.0\n",
    "\n",
    "    # We train the model for a number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model for one epoch\n",
    "        train_loss = train_epoch(model, trainloader, optimizer, criterion, epoch)\n",
    "\n",
    "        # For validation we do not keep track of gradients\n",
    "        with torch.no_grad():\n",
    "            val_accuracy, val_loss = valid_epoch(model, testloader, criterion, epoch)\n",
    "            # Save the best model based on validation accuracy\n",
    "            if val_accuracy > top_accuracy:\n",
    "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
    "                top_accuracy = val_accuracy\n",
    "\n",
    "        # Print a new line after each epoch\n",
    "        print()\n",
    "\n",
    "run_training(model, criterion, train_loader, valid_loader, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels, all_outputs = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_outputs.append(outputs.cpu())\n",
    "    \n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_outputs = torch.cat(all_outputs).numpy()\n",
    "    all_outputs = 1 / (1 + np.exp(-all_outputs))  # Apply sigmoid\n",
    "\n",
    "    # Convert outputs to binary predictions\n",
    "    predictions = (all_outputs > 0.5)\n",
    "\n",
    "    # Calculate multi-label accuracy\n",
    "    correct = (predictions == all_labels).sum()\n",
    "    total = np.prod(all_labels.shape)  # Total number of elements\n",
    "    accuracy = correct / total\n",
    "\n",
    "    f1 = f1_score(all_labels, predictions, average='macro')\n",
    "    roc_auc = roc_auc_score(all_labels, all_outputs, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7000\n",
      "F1 Score: 0.5776\n",
      "ROC AUC: 0.7383\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
